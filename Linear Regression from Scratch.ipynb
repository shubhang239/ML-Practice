{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model means finding the parameters that best fit the training dataset. To do this, we need to be able to measure how well the model fits the data. For linear regression, we find the value of Θ that minimizes the Mean Squared Error (MSE). There are multiple optimization algorithms to do this so we’ll look at a couple.\n",
    "Ordinary Least Squares\n",
    "The first method we’re going to code from scratch is called Ordinary Least Squares (OLS). OLS computes the pseudoinverse of X and multiplies it with the target values y. In python this method is pretty easy to implement using scipy.linalg.lstsq() which is the same function that Scikit-Learn’s LinearRegression() class uses.\n",
    "We’ll try and make ours similar to Scikit-Learn’s library by having fit() and predict() methods. We’ll also give it the attribute coef_ which is the parameters calculated during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import lstsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Class representing a linear regression model.\n",
    "    \n",
    "    Models relationship between target variables and attributes by computing line that \n",
    "    minimizes the mean squared error.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    solver : {lstsq}, \n",
    "        Optimization method used to minimize the mean squared error in training.\n",
    "        \n",
    "        'lstsq' : \n",
    "            Ordinary least squares method using scipy.linalg.lstsq\n",
    "            \n",
    "    Attributes  \n",
    "    -------------\n",
    "    \n",
    "    coef_ : array of shape (n_features, 1)\n",
    "        Estimated coefficients for the regression problem.\n",
    "        \n",
    "    Notes\n",
    "    -------------\n",
    "    This class is capable of being trained using ordinary least sqaures method.\n",
    "    See solver parameter above.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, solver = 'lstsq'):\n",
    "        self.solver = solver\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear regression model.\n",
    "        If solver = 'lstsq' model is trained using ordinary least squares.\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X: array like of shape (n_samples, n_features)\n",
    "           Training data. Independent variables.\n",
    "           \n",
    "        y: array like of shape (n_samples, 1)\n",
    "           Target values. Dependent variable.\n",
    "           \n",
    "        Returns\n",
    "        -------------\n",
    "        self: returns an instance of self.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.solver == 'lstsq':\n",
    "            # make sure inputs are numpy arrays\n",
    "            X = np.array(X)\n",
    "            y = np.array(y)\n",
    "            \n",
    "            # add x_0 = 1 to each instance for the bias term.\n",
    "            X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "            \n",
    "            # scipy implementation of least squares.\n",
    "            self.coef_, residues, rank, singular = lstsq(X,y)\n",
    "            \n",
    "            return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Estimate target values using the linear model.\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X: array-like of shape (n_samples, n_features) Instances.\n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        C: array of shape (n_samples, 1)\n",
    "           Estimated targets per instance\n",
    "        \"\"\"\n",
    "        # make sure input are numpy arrays\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Add x_0 = 1 to each instance for the bias term\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        return X.dot(self.coef_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To code the fit() method we simply add a bias term to our feature array and perform OLS with the function scipy.linalg.lstsq(). We store the calculated parameter coefficients in our attribute coef_ and then return an instance of self. The predict() method is even simpler. All we do is add a one to each instance for the bias term and then take the dot product of the feature matrix and our calculated parameters.\n",
    "The ordinary least squares algorithm can get very slow when the number of features grows very large. In these cases it is preferred to use another optimization approach called gradient descent.\n",
    "\n",
    "Batch Gradient Descent\n",
    "Gradient descent is a generic optimization algorithm that searches for the optimal solution by making small tweaks to the parameters. To start, you fill Θ with random values (this approach is called random initialization). Then, tweak the parameters until the algorithm converges to a minimum solution by traveling in a direction that decreases the cost function. In our case that means decreasing the MSE.\n",
    "To travel in the direction that decreases the cost function, you’re going to need to calculate the gradient vector which contains all the partial derivatives of the cost function.\n",
    "\n",
    "Equation 1. Gradient vector of the cost function\n",
    "After you calculate the gradient vector you update your parameters in the opposite direction that the gradient vector is pointing. This is where the learning rate (η) comes in to play. The learning rate determines the size of the steps you take in that direction.\n",
    "\n",
    "Equation 2. Gradient descent step\n",
    "Batch gradient descent is a version of gradient descent where we calculate the gradient vector of the entire dataset at each step. This means that batch gradient descent does not scale well with very large training sets because it has to load the entire dataset to calculate the next step. If you’re dataset is very large you might want to use stochastic gradient descent or mini-batch gradient descent, but we won’t cover those here. Just know they exist.\n",
    "Let’s edit our current module to include the option to use batch gradient descent for training. Since we’ll also be creating a Ridge, Lasso, and ElasticNet class, we’ll create a base Regression() class that all of our regressors can inherit from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.linalg import lstsq\n",
    "\n",
    "from mlscratch.utils.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class Regression():\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Class representing our base regression model.\n",
    "    \n",
    "    Models relationship between dependent scaler variable y and independent scaler variable X \n",
    "    by optimizing a cost function with batch gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    n_iter : float, defualt = 1000\n",
    "    \n",
    "        Maximum number of iterations to be used by batch gradient descent.\n",
    "        \n",
    "    lr : float, default = 1e-1\n",
    "    \n",
    "        Learning rate determining the size of steps in batch gradient descent algorithm.\n",
    "        \n",
    "    Attributes  \n",
    "    -------------\n",
    "    \n",
    "    coef_ : array of shape (n_features, 1)\n",
    "        Estimated coefficients for the regression problem.\n",
    "        \n",
    "    Notes\n",
    "    -------------\n",
    "    This class is capable of being trained using ordinary least sqaures method.\n",
    "    See solver parameter above.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_iter = 1000, lr = 1e-1):\n",
    "        \n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear regression model with batch gradient descent\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X: array like of shape (n_samples, n_features)\n",
    "           Training data. Independent variables.\n",
    "           \n",
    "        y: array like of shape (n_samples, 1)\n",
    "           Target values. Dependent variable.\n",
    "           \n",
    "        Returns\n",
    "        -------------\n",
    "        self: returns an instance of self.\n",
    "        \n",
    "        \"\"\"\n",
    "         \n",
    "            # make sure inputs are numpy arrays\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "            \n",
    "            # add x_0 = 1 to each instance for the bias term.\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "            \n",
    "            # store number of samples and features in variables.\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        self.training_errors = []\n",
    "            \n",
    "            # initialize weights randomly from normal distribution.\n",
    "        self.coef_ = np.random.randn(n_features, 1)\n",
    "            \n",
    "            # batch gradient descent for number of iterations = n_iter\n",
    "        for _ in range(self.n_iter):\n",
    "            y_preds = X.dot(self.coef_)\n",
    "                # calculate mse.\n",
    "            cost_function = mean_squared_error(y, y_preds)\n",
    "            self.training_errors.append(cost_function)\n",
    "                # Gradients of loss function\n",
    "            gradients = (2/n_samples)*X.T.dot(y_preds - y)\n",
    "            gradients = gradients\n",
    "                # update the weights\n",
    "            self.coef_ = self.lr*gradients\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Estimate target values using the linear model.\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X: array-like of shape (n_samples, n_features) Instances.\n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        C: array of shape (n_samples, 1)\n",
    "           Estimated targets per instance\n",
    "        \"\"\"\n",
    "        # make sure input are numpy arrays\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Add x_0 = 1 to each instance for the bias term\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        return X.dot(self.coef_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    \n",
    "    \"\"\"\"\n",
    "    Class representing a linear regression model.\n",
    "    \n",
    "    Models relationship between target variables and attributes by computing line that \n",
    "    minimizes the mean squared error.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    n_iter : float, defualt = 1000\n",
    "        Maximum number of iterations to be used by batch gradient descent\n",
    "        \n",
    "    lr: float, default = 1e-1\n",
    "        Learning rate determining the size of steps in batch gradient descent.\n",
    "        \n",
    "    solver : {lstsq, 'bgd'}, default = 'bgd'\n",
    "    \n",
    "        Optimization method used to minimize the mean squared error in training.\n",
    "        \n",
    "        \n",
    "        'lstsq' : \n",
    "            Ordinary least squares method using scipy.linalg.lstsq\n",
    "            \n",
    "        'bgd' : \n",
    "            Batch Gradient Descent.\n",
    "        \n",
    "    \n",
    "    Attributes  \n",
    "    -------------\n",
    "    \n",
    "    coef_ : array of shape (n_features, 1)\n",
    "        Estimated coefficients for the regression problem.\n",
    "        \n",
    "    Notes\n",
    "    -------------\n",
    "    This class is capable of being trained using ordinary least sqaures method\n",
    "    or batch gradient descent. See solver parameter above.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, solver = 'lstsq'):\n",
    "        self.solver = solver\n",
    "        super(LinearRegression, self).__init__(n_iter=n_iter, lr=lr)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear regression model.\n",
    "        If solver = 'lstsq' model is trained using ordinary least squares.\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X: array like of shape (n_samples, n_features)\n",
    "           Training data. Independent variables.\n",
    "           \n",
    "        y: array like of shape (n_samples, 1)\n",
    "           Target values. Dependent variable.\n",
    "           \n",
    "        Returns\n",
    "        -------------\n",
    "        self: returns an instance of self.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.solver == 'lstsq':\n",
    "            # make sure inputs are numpy arrays\n",
    "            X = np.array(X)\n",
    "            y = np.array(y)\n",
    "            \n",
    "            # add x_0 = 1 to each instance for the bias term.\n",
    "            X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "            \n",
    "            # scipy implementation of least squares.\n",
    "            self.coef_, residues, rank, singular = lstsq(X,y)\n",
    "            \n",
    "            return self\n",
    "        # if solver is bgd use batch gradient descent\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Estimate target values using the linear model.\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X: array-like of shape (n_samples, n_features) Instances.\n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        C: array of shape (n_samples, 1)\n",
    "           Estimated targets per instance\n",
    "        \"\"\"\n",
    "        # make sure input are numpy arrays\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Add x_0 = 1 to each instance for the bias term\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X.dot(self.coef_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a fully functional linear regression model capable of being trained using batch gradient descent and ordinary least squares. But, what if our data isn’t a straight line? If that’s the case we’ll need a more complex model that can fit nonlinear data like polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to model nonlinear data with a linear model is to add powers of each feature as new features, then train the model on this extended set of features. This technique is called polynomial regression.\n",
    "When you have multiple features, this technique is capable of finding relationships between features because you’re adding all combinations of features up to the given degree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we know polynomial regression is the same as linear regression except we add polynomial features to our dataset before training. Instead of creating a separate PolynomialRegression() class, we’ll add a preprocessing class that can transform your data before training. This way once we build our regularized linear models, they too will be able to perform polynomial regression.\n",
    "We’ll code it in a similar style to Scikit-Learn’s preprocessing classes. It will have a fit(), transform(), and fit_transform() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" Module containing classes for preprocessing data.\"\"\"\n",
    "from itertools import combinations_with_replacement\n",
    "import numpy as np\n",
    "from scipy.special import factorial\n",
    "\n",
    "class PolynomialFeatures:\n",
    "    \"\"\"\n",
    "    Generate Polynomial Features.\n",
    "    \n",
    "    Generate a new matrix of features including all combinations of different\n",
    "    polynomial features less than or equal to specified degree. No bias term is\n",
    "    calculated in this implementation because our regression models create a bias\n",
    "    term when they are trained.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    \n",
    "    degree : int, default=2\n",
    "        Degree of polynomial features to be created\n",
    "        \n",
    "    Attributes\n",
    "    -------------\n",
    "    n_input_features : int\n",
    "        The total number of input features.\n",
    "    n_output_features : int\n",
    "        The total number of output features computed by iterating over all possible\n",
    "        polynomial combinations of input features.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, degree = 2):\n",
    "        self.degree = degree\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Compute the number of output featurs.\n",
    "        \n",
    "            n_output_features = (n+d)!/d!n!\n",
    "        where n is the number of input features and d is the degree.\n",
    "        \n",
    "        Parameters \n",
    "        X: array-like of shape (n_samples, n_features)\n",
    "            Feature matrix to be transformed into polynomial feature matrix\n",
    "        \n",
    "        \"\"\"\n",
    "        # Make sure input is numpy array.\n",
    "        X = np.array(X)\n",
    "        # Store number of input features in attribute\n",
    "        self.n_input_features = X.shape[1]\n",
    "        # calculate numerator and denominator of equation listed above.\n",
    "        numerator = factorial(self.n_input_features + self.degree)\n",
    "        denominator = factorial(self.degree)*factorial(self.n_input_features)\n",
    "        # Calculate number of output features minus 1 to subtract bias term\n",
    "        self.n_output_features = int(numerator/denominator) - 1\n",
    "        \n",
    "\n",
    "def transform(self, X):\n",
    "    \"\"\"\"\n",
    "    Transform data to polynomial feature matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "            Feature matrix to be transformed into polynomial feature matrix.\n",
    "            \n",
    "    Returns\n",
    "    ------------\n",
    "    \n",
    "    X: array like of shape (n_samples, n_output_features)\n",
    "       Transformed polynomial feature matrix where n_output_features is\n",
    "       the number of output features after polynomial transformation.\n",
    "       \n",
    "       \"\"\"\n",
    "    # Generate all combinations of features indices and store them in tuples.\n",
    "    combos = [combinations_with_replacment(range(self.n_input_features), i)\n",
    "              for i in range(1, self.degree + 1)]\n",
    "    # Create a new array of the desired output shape\n",
    "    X_new = np.empty((X.shape[0], self.n_output_features))\n",
    "    # Multiply features for each combination tuple in combinations\n",
    "    for i, index_combos in enumerate(combinations):\n",
    "        X_new[:, i]=np.prod(X[:, index_combos], axis = 1)\n",
    "        \n",
    "    return X_new\n",
    "\n",
    "def fit_tranform(self, X):\n",
    "    \"\"\"\n",
    "    Compute the number of output features and transform to polynomial feature matrix.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    X: array like of shape (n_samples, n_features)\n",
    "    Feature matrix to be transformed into polynomial feature matrix.\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    X : array-like of shape (n_samples, n_output_features)\n",
    "        Transformed polynomial feature matrix where n_output_features is\n",
    "        the number of output features after polynomial transformation.\n",
    "        \n",
    "    \"\"\"\n",
    "    self.fit(X)\n",
    "    \n",
    "    return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try to perform polynomial regression at this point you might get an error during training. This is because you’re using gradient descent as the training algorithm. When we transform our features to add polynomial terms, it is very important we normalize our data if we’re using an iterative training algorithm like gradient descent. If we don’t, we risk encountering exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to normalize data, but we will stick with one of the simplest. By subtracting out the mean and dividing each instance by the standard deviation for each feature we effectively standardize our data. Meaning all our features are centered with a mean of 0 and unit variance.\n",
    "Let’s add a class called StandardScaler() to our preprocessing.py module. We’ll include an inverse_transform() method here in case we ever need to return data to its original state after it has been standardized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-3-48ad59d36b5f>, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-48ad59d36b5f>\"\u001b[1;36m, line \u001b[1;32m39\u001b[0m\n\u001b[1;33m    def transform(self, X):\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class StandardScaler:\n",
    "    \n",
    "    \"\"\"\n",
    "    Standardize features by centering the mean to 0 and unit variance.\n",
    "    \n",
    "    The standard score of an instance is calculated by:\n",
    "    \n",
    "    z =  (x-u)/s\n",
    "    \n",
    "    where u is the mean of the training data and s is the standard deviation.\n",
    "    \n",
    "    Standardizing data is often necessary before training many machine \n",
    "    learning models to avoid problems like exploding, vanishing gradients\n",
    "    and feature dominance.\n",
    "    \n",
    "    Attributes\n",
    "    ------------\n",
    "    mean_ : numpy array of shape (n_features, )\n",
    "        The mean of each feature in the training set.\n",
    "    var_ : numoy array of shape (n_features, )\n",
    "        The variance of each feature in the training set.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Calculate and store the mean and variance of each feature in the \n",
    "        training set.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X: array like of shape (n_samples, n_features)\n",
    "            Data set to calculate mean and variance of.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.mean_ = np.mean(X, axis = 0)\n",
    "        self.var_ = np.var(X, axis = 0)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Standardize data by subtracting out the mean and dividing by\n",
    "        standard deviation calculated during fitting.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        \n",
    "        X: array like of shape (n_samples, n_features)\n",
    "           Data to standardized.\n",
    "           \n",
    "        Returns\n",
    "        -----------\n",
    "        \n",
    "        X_std : array like of shape (n_samples, n_features)\n",
    "            Standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link: https://towardsdatascience.com/ml-from-scratch-linear-polynomial-and-regularized-regression-models-725672336076"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
